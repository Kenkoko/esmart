import: []

attention_top:
  class_name: AttentionLayer
  num_patches: 150
  projection_dim: 500
  transformer_layers: 1
  num_heads: 8
  dropout_rate: 0.3
  activation: softmax
  possition_embedding:
    regularize:
      type: l2
      penalty: 0.0001
    +++: +++
  layer_normalization:
    epsilon: 1e-6
    +++: +++